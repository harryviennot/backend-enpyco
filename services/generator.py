"""
Service for generating memoir sections using Claude API.
"""
from anthropic import Anthropic
from typing import List, Dict, Optional
from services.supabase import SupabaseService
from services.rag import RAGService
from config import Config


class GeneratorService:
    """Service for generating memoir sections using Claude API."""

    # Section type descriptions in French
    SECTION_DESCRIPTIONS = {
        'presentation': 'PrÃ©sentation de l\'entreprise (historique, chiffres clÃ©s, certifications)',
        'organisation': 'Organisation du chantier (PIC, moyens, logistique)',
        'methodologie': 'MÃ©thodologie de rÃ©alisation (phasage, techniques)',
        'moyens_humains': 'Moyens humains (organigramme, effectifs)',
        'moyens_materiels': 'Moyens matÃ©riels (liste Ã©quipements, capacitÃ©s)',
        'planning': 'Planning prÃ©visionnel (Gantt, dÃ©lais)',
        'environnement': 'DÃ©marche environnementale (RSE, gestion des dÃ©chets)',
        'securite': 'SÃ©curitÃ© et santÃ© (PPSPS, mesures de prÃ©vention)',
        'insertion': 'Insertion sociale (heures d\'insertion prÃ©vues)'
    }

    def __init__(self, api_key: str = None):
        """
        Initialize the generator service with Claude client.

        Args:
            api_key: Claude API key (defaults to Config.CLAUDE_API_KEY)
        """
        self.client = Anthropic(api_key=api_key or Config.CLAUDE_API_KEY)
        self.supabase = SupabaseService()
        self.rag = RAGService()
        self.model = "claude-sonnet-4-20250514"  # Latest Sonnet 4.5

        print(f"âœ… GeneratorService initialized with model: {self.model}")

    def generate_section(
        self,
        section_type: str,
        rc_context: str,
        reference_chunks: List[Dict],
        max_tokens: int = 4096,
        temperature: float = 0.7
    ) -> str:
        """
        Generate a memoir section using Claude API.

        Args:
            section_type: Type of section to generate (e.g., 'presentation', 'organisation')
            rc_context: Context extracted from RC document
            reference_chunks: List of similar chunks from RAG search
            max_tokens: Maximum tokens for generation (default: 4096)
            temperature: Temperature for generation (default: 0.7)

        Returns:
            Generated markdown content

        Raises:
            Exception: If generation fails
        """
        print(f"ðŸ“ Generating section: {section_type}")
        print(f"   Section description: {self.SECTION_DESCRIPTIONS.get(section_type, 'N/A')}")

        try:
            # Build prompt
            print(f"   ðŸ”¨ Building prompt...")
            prompt = self._build_prompt(section_type, rc_context, reference_chunks)

            # Log prompt size
            print(f"   âœ… Prompt built: ~{len(prompt)} characters")
            print(f"   ðŸ“š Reference chunks: {len(reference_chunks)}")
            print(f"   ðŸ“„ RC context: {len(rc_context)} characters")

            # Call Claude API
            print(f"   ðŸ¤– Calling Claude API (model: {self.model})...")
            print(f"   â³ This may take 20-60 seconds...")

            response = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            # Extract content
            content = response.content[0].text

            print(f"âœ… Section '{section_type}' generated successfully!")
            print(f"   ðŸ“Š Content length: {len(content)} characters")
            print(f"   ðŸ’° Tokens used: {response.usage.input_tokens} in / {response.usage.output_tokens} out")
            print(f"   â±ï¸  Total tokens: {response.usage.input_tokens + response.usage.output_tokens}")

            return content

        except Exception as e:
            print(f"âŒ Generation failed for section '{section_type}'")
            print(f"   Error type: {type(e).__name__}")
            print(f"   Error message: {str(e)}")
            import traceback
            traceback.print_exc()
            raise Exception(f"Failed to generate section '{section_type}': {str(e)}")

    def extract_rc_criteria(self, rc_text: str, max_tokens: int = 500) -> str:
        """
        Extract key criteria from RC document using Claude.

        This helps understand what the client is looking for in the memoir.

        Args:
            rc_text: Full or partial text from RC document
            max_tokens: Maximum tokens for extraction (default: 500)

        Returns:
            Extracted criteria as text

        Raises:
            Exception: If extraction fails
        """
        print(f"ðŸ” Extracting RC criteria")
        print(f"   RC text size: {len(rc_text)} characters")

        try:
            # Limit RC text to avoid token limits (take first 3000 chars)
            rc_sample = rc_text[:3000] if len(rc_text) > 3000 else rc_text

            prompt = f"""Analyse ce RÃ¨glement de Consultation et extrait les critÃ¨res d'Ã©valuation principaux du mÃ©moire technique.

RC :
{rc_sample}

Liste uniquement les 5-7 critÃ¨res les plus importants, de maniÃ¨re concise."""

            response = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=0.3,  # Lower temperature for extraction
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            criteria = response.content[0].text

            print(f"âœ… Criteria extracted: {len(criteria)} characters")

            return criteria

        except Exception as e:
            print(f"âŒ Criteria extraction failed: {type(e).__name__}: {str(e)}")
            raise Exception(f"Failed to extract RC criteria: {str(e)}")

    def _build_prompt(
        self,
        section_type: str,
        rc_context: str,
        reference_chunks: List[Dict]
    ) -> str:
        """
        Build the prompt for Claude API.

        Args:
            section_type: Type of section to generate
            rc_context: Context from RC document
            reference_chunks: List of reference chunks with 'content' and 'similarity'

        Returns:
            Formatted prompt string
        """
        # Get section description
        description = self.SECTION_DESCRIPTIONS.get(
            section_type,
            section_type.replace('_', ' ').title()
        )

        # Format reference chunks (top 5 by similarity)
        references_text = ""
        if reference_chunks:
            # Sort by similarity (descending) and take top 5
            sorted_chunks = sorted(
                reference_chunks,
                key=lambda x: x.get('similarity', 0),
                reverse=True
            )[:5]

            references_list = []
            for i, chunk in enumerate(sorted_chunks, 1):
                similarity = chunk.get('similarity', 0)
                content = chunk.get('content', '')
                metadata = chunk.get('metadata', {})
                filename = metadata.get('filename', 'Unknown')

                references_list.append(
                    f"Extrait de rÃ©fÃ©rence {i} (similaritÃ©: {similarity:.2f}, source: {filename}):\n{content}"
                )

            references_text = "\n\n---\n\n".join(references_list)
        else:
            references_text = "Aucune rÃ©fÃ©rence disponible. GÃ©nÃ¨re du contenu basÃ© sur les meilleures pratiques du BTP."

        # Format RC context
        rc_text = rc_context if rc_context else "Projet de construction (contexte non disponible)"

        # Build the full prompt
        prompt = f"""Tu es un expert en rÃ©daction de mÃ©moires techniques pour le BTP (BÃ¢timent et Travaux Publics).

**Contexte du projet (extrait du RÃ¨glement de Consultation) :**
{rc_text}

**Type de section Ã  gÃ©nÃ©rer :** {description}

**Contenu de rÃ©fÃ©rence (extraits de mÃ©moires similaires) :**
{references_text}

**Instructions :**
1. GÃ©nÃ¨re une section professionnelle de mÃ©moire technique
2. RÃ©utilise les informations factuelles des rÃ©fÃ©rences (chiffres, mÃ©thodes, Ã©quipements, certifications)
3. Adapte le contenu au contexte spÃ©cifique du RC
4. Utilise un ton professionnel mais accessible
5. PrivilÃ©gie les tableaux aux longues listes quand c'est pertinent
6. Structure : titre H2, sous-titres H3, paragraphes clairs
7. Utilise des bullet points pour les listes d'Ã©lÃ©ments

**Contraintes :**
- Format : Markdown
- Longueur : 500-1000 mots
- Ne pas inventer de donnÃ©es chiffrÃ©es, utiliser uniquement les rÃ©fÃ©rences
- Si une information n'est pas dans les rÃ©fÃ©rences, reste gÃ©nÃ©rique et professionnel
- Adapte les noms d'entreprise, projets, et dates au contexte du RC

GÃ©nÃ¨re maintenant la section :"""

        return prompt

    def get_valid_section_types(self) -> List[str]:
        """
        Get list of valid section types.

        Returns:
            List of section type identifiers
        """
        return list(self.SECTION_DESCRIPTIONS.keys())

    def validate_section_type(self, section_type: str) -> bool:
        """
        Check if a section type is valid.

        Args:
            section_type: Section type to validate

        Returns:
            True if valid, False otherwise
        """
        return section_type in self.SECTION_DESCRIPTIONS
